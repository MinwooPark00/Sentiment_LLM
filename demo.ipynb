{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e6e6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "‚úÖ [CHECKPOINT] Imports successful\n",
      "‚úÖ [SETUP] Hugging Face cache directory set to: /output/huggingface_cache\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# SETUP AND IMPORTS\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    from datasets import load_dataset\n",
    "    print(\"‚úÖ [CHECKPOINT] Imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ImportError: {e}\")\n",
    "    raise\n",
    "\n",
    "# Set Hugging Face cache directory to a larger, persistent volume\n",
    "cache_dir = \"/output/huggingface_cache\"\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "os.environ['HF_DATASETS_CACHE'] = os.path.join(cache_dir, \"datasets\")\n",
    "os.environ['TRANSFORMERS_CACHE'] = os.path.join(cache_dir, \"models\")\n",
    "\n",
    "# Prevent tokenizer parallelism issues\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Ensure the cache directory exists\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "print(f\"‚úÖ [SETUP] Hugging Face cache directory set to: {cache_dir}\")\n",
    "\n",
    "# --- Configuration ---\n",
    "STUDENT_MODEL_NAME = \"unsloth/Llama-3.2-1B-unsloth-bnb-4bit\"\n",
    "ADAPTER_PATH = \"./train_outputs/sst2_finetune/final_adapter\" \n",
    "BEST_RESULT_PATH = \"./optimization_results/best_result.json\"\n",
    "STUDENT_MAX_SEQ_LENGTH = 8192\n",
    "DEMO_SAMPLE_INDEX = 1\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c0cee8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Student Model: unsloth/Llama-3.2-1B-unsloth-bnb-4bit...\n",
      "==((====))==  Unsloth 2025.6.2: Fast Llama patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.684 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Loading adapter from: ./train_outputs/sst2_finetune/final_adapter\n",
      "‚úÖ [CHECKPOINT] Student model and adapter loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# LOAD FINE-TUNED STUDENT MODEL\n",
    "\n",
    "print(f\"\\nLoading Student Model: {STUDENT_MODEL_NAME}...\")\n",
    "try:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=STUDENT_MODEL_NAME,\n",
    "        max_seq_length=STUDENT_MAX_SEQ_LENGTH,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    print(f\"Loading adapter from: {ADAPTER_PATH}\")\n",
    "    model.load_adapter(ADAPTER_PATH)\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    print(\"‚úÖ [CHECKPOINT] Student model and adapter loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load student model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb07e801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading best instruction from ./optimization_results/best_result.json...\n",
      "‚úÖ [CHECKPOINT] Best instruction loaded successfully.\n",
      "\n",
      "--- Best Instruction ---\n",
      "Classify the overall sentiment of the following movie review as either \"Positive\" or \"Negative\".\n",
      "To make this classification, consider the entire text, not just a single sentence.\n",
      "If the review expresses a general attitude that is overwhelmingly positive (e.g., praising the movie, enjoying the experience), output \"Positive\".\n",
      "If the review expresses a general attitude that is overwhelmingly negative (e.g., criticizing the movie, disliking the experience), output \"Negative\".\n",
      "When dealing with mixed reviews that mention both positive and negative aspects, look for the overall tone of the review.\n",
      "If the review's overall tone is more positive than negative, output \"Positive\".\n",
      "If the review's overall tone is more negative than positive, output \"Negative\".\n",
      "If the review is neutral, meaning it doesn't express a clear positive or negative attitude, output \"Neutral\".\n"
     ]
    }
   ],
   "source": [
    "# LOAD THE BEST OPTIMIZED INSTRUCTION\n",
    "    \n",
    "print(f\"\\nLoading best instruction from {BEST_RESULT_PATH}...\")\n",
    "try:\n",
    "    with open(BEST_RESULT_PATH, 'r') as f:\n",
    "        best_result_data = json.load(f)\n",
    "    best_instruction = best_result_data[\"best_instruction\"]\n",
    "    print(\"‚úÖ [CHECKPOINT] Best instruction loaded successfully.\")\n",
    "    print(\"\\n--- Best Instruction ---\")\n",
    "    # Print sentences separately\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', best_instruction)\n",
    "    for s in sentences:\n",
    "        print(s)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load best instruction file: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf441d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading a sample from IMDb dataset...\n",
      "‚úÖ [CHECKPOINT] Demo sample (Index: 1) loaded.\n",
      "\n",
      "--- Sample Review Snippet ---\n",
      "Worth the entertainment value of a rental, especially if you like action movies.\n",
      "This one features the usual car chases, fights with the great Van Damme kick style, shooting battles with the 40 shell load shotgun, and even terrorist style bombs.\n",
      "All of this is entertaining and competently handled but there is nothing that really blows you away if you've seen your share before.<br /><br />The plot is made interesting by the inclusion of a rabbit, which is clever but hardly profound.\n",
      "Many of the characters are heavily stereotyped -- the angry veterans, the terrified illegal aliens, the crooked cops, the indifferent feds, the bitchy tough lady station head, the crooked politician, the fat federale who looks like he was typecast as the Mexican in a Hollywood movie from the 1940s.\n",
      "All passably acted but again nothing special.<br /><br />I thought the main villains were pretty well done and fairly well acted.\n",
      "By the end of the movie you certainly knew who the good guys were and weren't.\n",
      "There was an emotional lift as the really bad ones got their just deserts.\n",
      "Very simplistic, but then you weren't expecting Hamlet, right?\n",
      "The only thing I found really annoying was the constant cuts to VDs daughter during the last fight scene.<br /><br />Not bad.\n",
      "Not good.\n",
      "Passable 4.\n",
      "\n",
      "(Ground Truth Label: Negative)\n"
     ]
    }
   ],
   "source": [
    "# LOAD A DEMO SAMPLE\n",
    "\n",
    "print(\"\\nLoading a sample from IMDb dataset...\")\n",
    "try:\n",
    "    # Load just one sample to keep it fast\n",
    "    demo_sample = load_dataset(\"imdb\", split=f\"test[{DEMO_SAMPLE_INDEX}:{DEMO_SAMPLE_INDEX+1}]\")[0]\n",
    "    label_map = {0: \"Negative\", 1: \"Positive\"}\n",
    "    demo_text = demo_sample[\"text\"]\n",
    "    demo_true_label = label_map.get(demo_sample[\"label\"])\n",
    "    \n",
    "    print(f\"‚úÖ [CHECKPOINT] Demo sample (Index: {DEMO_SAMPLE_INDEX}) loaded.\")\n",
    "    print(\"\\n--- Sample Review Snippet ---\")\n",
    "    # Print sentences separately\n",
    "    for sentence in re.split(r'(?<=[.!?])\\s+', demo_text):\n",
    "        print(sentence)\n",
    "    print(f\"\\n(Ground Truth Label: {demo_true_label})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load IMDb dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b30c221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> CONSTRUCTING FINAL PROMPT...\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Classify the overall sentiment of the following movie review as either \"Positive\" or \"Negative\". To make this classification, consider the entire text, not just a single sentence. If the review expresses a general attitude that is overwhelmingly positive (e.g., praising the movie, enjoying the experience), output \"Positive\". If the review expresses a general attitude that is overwhelmingly negative (e.g., criticizing the movie, disliking the experience), output \"Negative\". When dealing with mixed reviews that mention both positive and negative aspects, look for the overall tone of the review. If the review's overall tone is more positive than negative, output \"Positive\". If the review's overall tone is more negative than positive, output \"Negative\". If the review is...\n",
      "\n",
      ">>> RUNNING INFERENCE...\n",
      "‚úÖ Inference complete.\n",
      "\n",
      "==================================================\n",
      "=== FINAL RESULT ===\n",
      "==================================================\n",
      "Model Raw Output: 'Negative'\n",
      "Parsed Prediction: Negative\n",
      "Ground Truth:      Negative\n",
      "\n",
      "Outcome: ‚úÖ Correct\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# RUN DEMONSTRATION\n",
    "\n",
    "# --- Define Prompt Structure ---\n",
    "BASE_PROMPT_TEMPLATE = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{review}\n",
    "\n",
    "### Response:\n",
    "\"\"\" # Note: removed {response} placeholder for inference\n",
    "\n",
    "# --- Construct the full prompt and run inference ---\n",
    "final_prompt = BASE_PROMPT_TEMPLATE.format(\n",
    "    instruction=best_instruction,\n",
    "    review=demo_text,\n",
    ")\n",
    "\n",
    "print(f\"\\n>>> CONSTRUCTING FINAL PROMPT...\")\n",
    "time.sleep(1) # Pause for demo effect\n",
    "print(final_prompt[:900] + \"...\")\n",
    "\n",
    "\n",
    "print(f\"\\n>>> RUNNING INFERENCE...\")\n",
    "inputs = tokenizer(final_prompt, return_tensors=\"pt\", truncation=True, max_length=STUDENT_MAX_SEQ_LENGTH).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=5, pad_token_id=tokenizer.eos_token_id)\n",
    "prediction_text = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True).strip()\n",
    "print(\"‚úÖ Inference complete.\")\n",
    "\n",
    "\n",
    "# --- Display final result ---\n",
    "predicted_label = \"Positive\" if \"Positive\" in prediction_text else \"Negative\" if \"Negative\" in prediction_text else \"Unknown\"\n",
    "\n",
    "print(\"\\n==================================================\")\n",
    "print(\"=== FINAL RESULT ===\")\n",
    "print(\"==================================================\")\n",
    "print(f\"Model Raw Output: '{prediction_text}'\")\n",
    "print(f\"Parsed Prediction: {predicted_label}\")\n",
    "print(f\"Ground Truth:      {demo_true_label}\")\n",
    "print(f\"\\nOutcome: {'‚úÖ Correct' if predicted_label == demo_true_label else '‚ùå Incorrect'}\")\n",
    "print(\"==================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

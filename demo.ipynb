{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e6e6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP AND IMPORTS\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    from datasets import load_dataset\n",
    "    print(\"✅ [CHECKPOINT] Imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ ImportError: {e}\")\n",
    "    raise\n",
    "\n",
    "# Set Hugging Face cache directory to a larger, persistent volume\n",
    "cache_dir = \"/output/huggingface_cache\"\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "os.environ['HF_DATASETS_CACHE'] = os.path.join(cache_dir, \"datasets\")\n",
    "os.environ['TRANSFORMERS_CACHE'] = os.path.join(cache_dir, \"models\")\n",
    "\n",
    "# Prevent tokenizer parallelism issues\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Ensure the cache directory exists\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "print(f\"✅ [SETUP] Hugging Face cache directory set to: {cache_dir}\")\n",
    "\n",
    "# --- Configuration ---\n",
    "STUDENT_MODEL_NAME = \"unsloth/Llama-3.2-1B-unsloth-bnb-4bit\"\n",
    "ADAPTER_PATH = \"./train_outputs/sst2_finetune/final_adapter\" \n",
    "BEST_RESULT_PATH = \"./optimization_results/best_result.json\"\n",
    "STUDENT_MAX_SEQ_LENGTH = 8192\n",
    "DEMO_SAMPLE_INDEX = 1\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0cee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FINE-TUNED STUDENT MODEL\n",
    "\n",
    "print(f\"\\nLoading Student Model: {STUDENT_MODEL_NAME}...\")\n",
    "try:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=STUDENT_MODEL_NAME,\n",
    "        max_seq_length=STUDENT_MAX_SEQ_LENGTH,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    print(f\"Loading adapter from: {ADAPTER_PATH}\")\n",
    "    model.load_adapter(ADAPTER_PATH)\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    print(\"✅ [CHECKPOINT] Student model and adapter loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to load student model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb07e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD THE BEST OPTIMIZED INSTRUCTION\n",
    "    \n",
    "print(f\"\\nLoading best instruction from {BEST_RESULT_PATH}...\")\n",
    "try:\n",
    "    with open(BEST_RESULT_PATH, 'r') as f:\n",
    "        best_result_data = json.load(f)\n",
    "    best_instruction = best_result_data[\"best_instruction\"]\n",
    "    print(\"✅ [CHECKPOINT] Best instruction loaded successfully.\")\n",
    "    print(\"\\n--- Best Instruction ---\")\n",
    "    # Print sentences separately\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', best_instruction)\n",
    "    for s in sentences:\n",
    "        print(s)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to load best instruction file: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf441d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD A DEMO SAMPLE\n",
    "\n",
    "print(\"\\nLoading a sample from IMDb dataset...\")\n",
    "try:\n",
    "    # Load just one sample to keep it fast\n",
    "    demo_sample = load_dataset(\"imdb\", split=f\"test[{DEMO_SAMPLE_INDEX}:{DEMO_SAMPLE_INDEX+1}]\")[0]\n",
    "    label_map = {0: \"Negative\", 1: \"Positive\"}\n",
    "    demo_text = demo_sample[\"text\"]\n",
    "    demo_true_label = label_map.get(demo_sample[\"label\"])\n",
    "    \n",
    "    print(f\"✅ [CHECKPOINT] Demo sample (Index: {DEMO_SAMPLE_INDEX}) loaded.\")\n",
    "    print(\"\\n--- Sample Review Snippet ---\")\n",
    "    # Print sentences separately\n",
    "    for sentence in re.split(r'(?<=[.!?])\\s+', demo_text):\n",
    "        print(sentence)\n",
    "    print(f\"\\n(Ground Truth Label: {demo_true_label})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to load IMDb dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b30c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN DEMONSTRATION\n",
    "\n",
    "# --- Define Prompt Structure ---\n",
    "BASE_PROMPT_TEMPLATE = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{review}\n",
    "\n",
    "### Response:\n",
    "\"\"\" # Note: removed {response} placeholder for inference\n",
    "\n",
    "# --- Construct the full prompt and run inference ---\n",
    "final_prompt = BASE_PROMPT_TEMPLATE.format(\n",
    "    instruction=best_instruction,\n",
    "    review=demo_text,\n",
    ")\n",
    "\n",
    "print(f\"\\n>>> CONSTRUCTING FINAL PROMPT...\")\n",
    "time.sleep(1) # Pause for demo effect\n",
    "print(final_prompt[:900] + \"...\")\n",
    "\n",
    "\n",
    "print(f\"\\n>>> RUNNING INFERENCE...\")\n",
    "inputs = tokenizer(final_prompt, return_tensors=\"pt\", truncation=True, max_length=STUDENT_MAX_SEQ_LENGTH).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=5, pad_token_id=tokenizer.eos_token_id)\n",
    "prediction_text = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True).strip()\n",
    "print(\"✅ Inference complete.\")\n",
    "\n",
    "\n",
    "# --- Display final result ---\n",
    "predicted_label = \"Positive\" if \"Positive\" in prediction_text else \"Negative\" if \"Negative\" in prediction_text else \"Unknown\"\n",
    "\n",
    "print(\"\\n==================================================\")\n",
    "print(\"=== FINAL RESULT ===\")\n",
    "print(\"==================================================\")\n",
    "print(f\"Model Raw Output: '{prediction_text}'\")\n",
    "print(f\"Parsed Prediction: {predicted_label}\")\n",
    "print(f\"Ground Truth:      {demo_true_label}\")\n",
    "print(f\"\\nOutcome: {'✅ Correct' if predicted_label == demo_true_label else '❌ Incorrect'}\")\n",
    "print(\"==================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
